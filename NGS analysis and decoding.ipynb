{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"yTt8h9XTuAs2"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount = True)\n","nb_path = '/content/drive/MyDrive/Colab Notebooks'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uuas1kYRQy36"},"outputs":[],"source":["!pip install Bio"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5m4kSg-VDAUj"},"outputs":[],"source":["import os\n","os.chdir(\"/content/drive/MyDrive\")\n","\n","import scipy\n","import pandas as pd\n","from Bio import SeqIO\n","from Bio.Seq import Seq\n","import glob\n","\n","os.chdir('/content/drive/MyDrive/Colab Notebooks')\n","import matplotlib.pyplot as plt\n","import numpy as np\n","# from fuzzywuzzy import process\n","import gzip\n","from rapidfuzz import process"]},{"cell_type":"markdown","metadata":{"id":"NACGovWFu-UM"},"source":["# Installation things (run if you are a new user)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-s7w0_dPuNjU"},"outputs":[],"source":["import os\n","import sys\n","\n","nb_path = '/content/drive/MyDrive'\n","if nb_path not in sys.path:\n","  sys.path.insert(5, nb_path)\n","# os.symlink('/content/notebooks', nb_path)\n","os.chdir(\"/content/drive/MyDrive\")\n","\n","#pip install --target=$nb_path abifpy #change package name here"]},{"cell_type":"markdown","metadata":{"id":"pAN8IV3HvB41"},"source":["# Import files"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m33-zQM5-usj"},"outputs":[],"source":["import glob\n","# rewrite all files in the dir as .fastq, change read1/2 as needed\n","\n","dir = '/content/drive/MyDrive/Colab Notebooks/FolderName'\n","# save to fastq\n","files = glob.glob((dir + \"/*READ1-Sequences.txt.gz\"))\n","for f in files:\n","  s = str(os.path.basename(f))\n","  print(s)\n","  s = s.replace('.txt.gz','')\n","  with gzip.open(f, \"rt\") as handle:\n","    records = (rec for rec in SeqIO.parse(handle, \"fastq\"))\n","    count = SeqIO.write(records, str(s +\".fastq\"), \"fastq\")\n","    print(count)# rename and re-write all files as fastq\n","  handle.close()\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BAh-1Ki21C-J"},"source":["# pair Read1 + Read2\n","\n","Pairs read1 and read2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oduU9RNVGsmT"},"outputs":[],"source":["#set working directory\n","dir = \"/content/drive/MyDrive/Colab Notebooks/\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ze4ddOb47OtZ"},"outputs":[],"source":["# from Bio import SeqIO\n","# files = glob.glob(dir + \"/*.fastq\")\n","# for file in files:\n","# Get the lengths and ids, and sort on length\n","# base=os.path.basename(file)\n","# file_name = os.path.splitext(base)[0]\n","# file_name = os.path.splitext(file_name)[0]\n","# print(file_name)\n","with gzip.open(dir+file_name +\"_merged1_fastq\", \"rt\") as handle:\n","  records = (rec for rec in SeqIO.parse(handle, \"fastq\"))\n","  count = SeqIO.write(records, str(file_name+\".fastq\"), \"fastq\")\n","print(count)\n","handle.close()\n"]},{"cell_type":"markdown","metadata":{"id":"UfgMbsilQyYl"},"source":["merge fastq files"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Ms-ImnA86Pe"},"outputs":[],"source":["file_name = 'mR417-L1-P4-ACAGTG'\n","barcode = 'ACAGTG'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FXfxY7lRCmq7"},"outputs":[],"source":["from Bio.SeqIO.QualityIO import FastqGeneralIterator\n","import itertools\n","\n","#Setup variables (could parse command line args instead)\n","file_f = file_name+\"-READ1-Sequences.fastq\"\n","file_r = file_name+\"-READ2-Sequences.fastq\"\n","file_out = file_name+\"_merge.fastq\"\n","format = \"fastq\" #or \"fastq-illumina\", or \"fasta\", or ...\n","\n","\n","def interleave(iter1, iter2) :\n","    for (forward, reverse) in zip(iter1,iter2):\n","        assert forward.id == reverse.id\n","        forward.id += \"/1\"\n","        reverse.id += \"/2\"\n","        yield forward\n","        yield reverse\n","\n","records_f = SeqIO.parse(open(file_f,\"r\"), format)\n","records_r = SeqIO.parse(open(file_r,\"r\"), format)\n","\n","handle = open(file_out, \"w\")\n","count = SeqIO.write(interleave(records_f, records_r), handle, format)\n","print(count)\n","handle.close()\n"]},{"cell_type":"markdown","metadata":{"id":"CQelFodXQ1d4"},"source":["PhredFilter only > 20\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PYqy5LZNMPCx"},"outputs":[],"source":["# save to csv, only quality reads\n","\n","df = pd.DataFrame(columns = ['Seq', 'Line#', \"PhredScore\"])\n","records=[]\n","for i, rec in enumerate(SeqIO.parse(file_name+\"-READ1-Sequences.fastq\", \"fastq\")):\n","  df.loc[len(df)] = [str(rec.seq), i+1, score]\n","  if min(rec.letter_annotations[\"phred_quality\"]) >= 20:\n","    score = rec.letter_annotations[\"phred_quality\"]\n","    records.append(rec)\n","    df.loc[len(df)] = [str(rec.seq), i+1, score]\n","    # print(df)\n","print(len(df))\n","df.to_csv(\"allseqs_\"+barcode+\".csv\")\n","count = SeqIO.write(records, str(file_name+\"_quality.fastq\"), \"fastq\")\n","print(count)"]},{"cell_type":"code","source":["plt.hist(all_scores, bins = 100)\n","plt.xlabel ('Phred Score')\n","plt.ylabel('Count')"],"metadata":{"id":"JlOQ22zzMj0Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Begin analysis of fastq files merged using PEAR"],"metadata":{"id":"C-tVM4Nlq8Rr"}},{"cell_type":"markdown","metadata":{"id":"991GOqOm1Ew4"},"source":["## Decoding sequence to position-based indexers\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7E5OIjM3MAwB"},"outputs":[],"source":["file_name = \"/content/drive/MyDrive/Colab Notebooks/NEW DEL19 FILES/20240221 DEL19 Cell Screen 1/mR417-L1-P1-ATGTCA\"\n","barcode= \"ATCACG\"\n","mm= 0"]},{"cell_type":"markdown","metadata":{"id":"K9vClu1PyGSX"},"source":["##trim and save encoding regions by position index\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zUZhl7d5yJl3"},"outputs":[],"source":["from Bio import Align\n","import io\n","import csv\n","from csv import writer\n","import rapidfuzz\n","\n","seqs = pd.DataFrame()\n","decoding_table = pd.read_csv('6_Pos_Oligos_NGS_Fwd.csv')\n","\n","df = pd.DataFrame(columns = ['Pos1', 'Pos2', 'Pos3', 'Pos4', 'Pos5', 'Pos6', 'UMI','Line#'])\n","array = np.empty((len(seqs), 8))\n","\n","output = io.StringIO()\n","writer = csv.writer(output, quoting=csv.QUOTE_NONNUMERIC)\n","writer.writerow(np.array([1,2,3,4,5,6,7,8]))\n","\n","for i, read in enumerate(SeqIO.parse(file_name+\".fastq\", \"fastq\")):\n","\n","\n","  ref='GCCGCCGCCTTCGTCCTTCTCAGCGACATGGxxxxxxxxTCAxxxxxxxxGTTxxxxxxxxCTAxxxxxxxxTTCxxxxxxxxCGCxxxxxxxxGCCTCCCAAACxxxxxxxxGTTTGCCCGCCAGTTGTTGTGCCAC'\n","\n","  aligner = Align.PairwiseAligner()\n","  aligner.mismatch_score = -1\n","  aligner.internal_gap_score = -1\n","  alignments = aligner.align(ref, read)\n","\n","\n","  i1 = 28\n","  i2 = 39\n","  add_array = []\n","  while i2 < 95:\n","    o1 = str(read.seq[i1:i2])\n","    best = rapidfuzz.process.extract(o1, decoding_table['Sequences'], score_cutoff = 95, limit = 1)\n","\n","    if best:#if match was found\n","\n","      oligo_code = decoding_table.loc[decoding_table['Sequences'] == str(best[0][0])]['Tag'].item()\n","\n","    else:\n","      oligo_code = \"\"\n","    add_array.append(str(oligo_code))\n","    i1 = i2\n","    i2 += 11\n","\n","  add_array.append(read.seq[105:113])\n","  add_array.append(i)\n","  #print(add_array)\n","  #UMI\n","  if add_array[0][0:2] == \"11\" and add_array[1][0:2] == \"22\" and add_array[2][0:2] == \"13\" and add_array[3][0:2] == \"24\" and add_array[4][0:2] == \"15\" and add_array[5][0:2] == \"26\":\n","    writer.writerow(add_array)\n","  if i%100 == 0:\n","    print(i)\n","\n","output.seek(0) # we need to get back to the start of the BytesIO\n","df = pd.read_csv(output, dtype = str)\n","df.columns = ['Pos1', 'Pos2', 'Pos3', 'Pos4', 'Pos5', 'Pos6', 'UMI','Line#']\n","print(df)\n","\n","df['Code'] = list(zip(df.Pos1, df.Pos2, df.Pos3,df.Pos4,df.Pos5, df.Pos6 ))\n","#print(df['new'])\n","df.to_csv('rapid_proc_%s_decodes_%iMM.csv' % (barcode, mm))"]},{"cell_type":"markdown","metadata":{"id":"Nn8JWmj6qUHY"},"source":["##Group \"beads\" and collect Unique UMI's for each bead"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VrxQ8bzvqaNs"},"outputs":[],"source":["barcode= \"ATCACG\"\n","mm= 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-4CvkLWNDiVM"},"outputs":[],"source":["# collapse all reads and find unique UMI's for them.\n","\n","df = pd.read_csv('rapid_proc_%s_decodes_%iMM.csv' % (barcode, mm))#\"Code_matches_\"+barcode+\"_2MM.csv\")\n","df = df.replace('nan', np.nan).dropna(axis=0)\n","\n","\n","UMIenriched_df = pd.DataFrame(df.groupby(['Code','UMI']).nunique()).reset_index()\n","print(UMIenriched_df)\n","# UMIenriched_df.columns = ['Code', 'UMI', 'UMICount', 'ReadCount']\n","print(UMIenriched_df)\n","UMIenriched_df.to_csv(barcode +\"_perumicount.csv\")#\"collapsed_reads_perUMIcount_\"+barcode+\"_2MM.csv\")\n","\n","grouped_df = pd.DataFrame(df.groupby(['Code'], as_index = False).agg(UMI_unique=('UMI', 'unique'),UMI_count=('UMI', 'count'), Linecount=('Line#','count')).reset_index())\n","\n","print(grouped_df)\n","grouped_df.to_csv(barcode +\" groupedUMI.csv\")#collapsed_reads_groupedUMI_\"+barcode+\"_2MM.csv\")\n","\n","\n","\n","counts_df = pd.DataFrame(df.groupby('Code')[['UMI','Line#']].nunique())\n","counts_df.columns = ['UniqueUMI','UniqueLine#']\n","counts_df.to_csv(barcode +\"umicounts.csv\")#collapsed_reads_UMIcounts_\"+barcode+\"_2MM.csv\")"]},{"cell_type":"markdown","metadata":{"id":"RaBu2FHLJ1P8"},"source":["Conduct UMI string condensation to condense any UMIs which are only one nucleotide apart (HD = 1) and may be a result of mutation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"djOa_A71QtjO"},"outputs":[],"source":["df = pd.read_csv('rapid_proc_%s_decodes_%iMM.csv' % (barcode, mm))\n","# display(df)\n","test_df= df.groupby(['Code'], as_index = False).agg(UMI_unique=('UMI', 'unique'),UMI_count=('UMI', 'count'), Linecount=('Line#','count')).reset_index()\n","\n","display(test_df)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BTCc3lKdPMBH"},"outputs":[],"source":["# test_df = pd.read_csv(barcode +\" groupedUMI.csv\")\n","import math\n","def hamming(s1, s2):\n","    if len(s1) != len(s2):\n","        raise ValueError(\"Undefined for sequences of unequal length\")\n","    return sum(ch1 != ch2 for ch1, ch2 in zip(s1, s2))\n","\n","test_df['UMIs_condensed'] = \"\"\n","test_df['UMIs_condensed_count'] = 0\n","\n","for idx,row in test_df.iterrows():\n","  result = []\n","  pieces = row['UMI_unique']\n","\n","  if row['UMI_count'] >= 2:\n","\n","    pieces = row['UMI_unique']\n","    # print(pieces)\n","    # print(len(pieces))\n","    thischunk=[]\n","    for piece in pieces:\n","      if pd.isna(piece) or len(piece)!=8:\n","        continue\n","      if all(hamming(piece, base) > 1 for base in thischunk):\n","            thischunk.append(piece)\n","    result = thischunk\n","\n","    if len(pieces)>len(result):\n","      print(\"original:\")\n","      print(pieces)\n","      print(len(pieces))\n","      print(\"result:\")\n","      print(result)\n","      print(len(result))\n","\n","      test_df.at[idx, 'UMIs_condensed'] = result\n","      test_df.at[idx, 'UMIs_condensed_count'] = len(result)\n","    else:\n","      test_df.at[idx, 'UMIs_condensed'] = pieces\n","      test_df.at[idx, 'UMIs_condensed_count'] = len(pieces)\n","  else:\n","    test_df.at[idx, 'UMIs_condensed'] = pieces\n","    test_df.at[idx, 'UMIs_condensed_count'] = len(pieces)\n","\n","display(test_df)\n","test_df.to_csv('rapid_proc_%s_decodes_%iMM_UMIcondensed.csv' % (barcode, mm))"]},{"cell_type":"markdown","metadata":{"id":"UIaOP_G3uA1u"},"source":["##Match encoding regions to BBs using a user-provided .csv files with BB assignments"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"57ESzLvLJ04i"},"outputs":[],"source":["#Example code reflecting how encoding codes are matched to building blocks provided in a .csv by the user\n","\n","df = pd.read_csv('rapid_proc_%s_decodes_%iMM_UMIcondensed.csv' % (barcode, mm))\n","# df = df.replace('unique', np.nan).dropna(axis=0)\n","\n","df= df.replace(\"\\(\\'\", '[', regex = True)\n","df= df.replace('\\'\\)', ']', regex = True)\n","df= df.replace(\"\\'\", '', regex = True)\n","print(df)\n","df['Code'] = df['Code'].apply(eval)\n","codes = pd.read_csv('coding scheme DEL19.csv', dtype = str)\n","#print(codes['code 1'])\n","new_df = pd.DataFrame(columns = ['BB code', 'sequence', 'UMI', 'UMIcount','Line#', 'ER1', 'ER2', 'ER3', 'ER4', 'ER5', 'ER6'])\n","c = 0\n","for count, entry in enumerate(df['Code'].values):\n","  # print(entry)\n","  #print(len(df.iloc[c, 1]))\n","  pos1 = entry[0]\n","  pos2 = entry[1]\n","  pos3 = entry[2]\n","  pos4 = entry[3]\n","  pos5 = entry[4]\n","  pos6 = entry[5]\n","\n","  BSB = str(entry[0]) + \"-\" + str(entry[1])\n","  code1 =str(entry[2])+ \"-\" + str(entry[3])\n","  code2 =str(entry[4]) + \"-\" + str(entry[5])\n","\n","  bb1 = codes[codes.eq(code1).any(axis=1)]['BB code'].values\n","  bb2 = codes[codes.eq(code2).any(axis=1)]['BB code'].values\n","  # (print(bb1,bb2))\n","  if bb1 != \"\" and bb2 != \"\": #only reads with full length matches will continue now.\n","    print(bb1,bb2)\n","    new_df.loc[len(new_df)] = [bb1, entry, df.iloc[c]['UMIs_condensed'], df.iloc[c]['UMIs_condensed_count'], df.iloc[c]['Linecount'],\n","                               entry[0],entry[1],entry[2],entry[3],entry[4],entry[5]]\n","    new_df.loc[len(new_df)] = [bb2, entry, df.iloc[c]['UMIs_condensed'], df.iloc[c]['UMIs_condensed_count'], df.iloc[c]['Linecount'],\n","                               entry[0],entry[1],entry[2],entry[3],entry[4],entry[5]]\n","    #print(new_df)\n","    c += 1\n","print(new_df)\n","new_df.to_csv(\"rapid_%s_BB_match.csv\" % barcode)"]},{"cell_type":"markdown","metadata":{"id":"61Qvf_tVlgdB"},"source":["##UMI trimming"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5wt662WlAVZ0"},"outputs":[],"source":["# Plot UMI's to observe distribution of counts. This should help inform the basis of the UMI cutoff.\n","\n","# Plot as Log scale, Y axis\n","\n","df= pd.read_csv(\"rapid_%s_BB_match.csv\" % (barcode))\n","binwidth = 10\n","plt.hist(df['UMIcount'],\n","         bins = range(1, 1000 + binwidth, binwidth)\n","          ,log = True)\n","plt.title(\"UMI counts, \" + barcode)\n","plt.xscale('log')\n","\n","plt.show()"]},{"cell_type":"code","source":["# Plot as linear scale, Y axis\n","\n","\n","df= pd.read_csv(\"rapid_%s_BB_match.csv\" % (barcode))\n","binwidth = 10\n","plt.hist(df['UMIcount'],\n","         bins = range(1, 1000 + binwidth, binwidth))\n","plt.title(\"UMI counts, \" + barcode)\n","plt.xscale('log')\n","\n","plt.show()"],"metadata":{"id":"9brBu3NEPg9Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jjkR87rA6gYQ"},"source":["Trim files by UMI cutoff"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fzDagvaK5aIE"},"outputs":[],"source":["#set cutoff here\n","UMI_cutoff = 10\n","\n","## code\n","df = pd.read_csv(\"rapid_%s_BB_match.csv\" % (barcode))\n","df = df[df['UMIcount'] >= UMI_cutoff]\n","print(df)\n","df.to_csv(\"rapid_%s_BB_match_trimmed.csv\" % (barcode))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RcSe1SK_Dg-0"},"outputs":[],"source":["#take umi filtered beads and collapse on each BB\n","df = pd.read_csv(\"rapid_%s_BB_match_trimmed.csv\" % (barcode))\n","df2 = df.groupby('BB code')[['sequence', 'ER1', 'ER2', 'ER3','ER4','ER5','ER6']].nunique()\n","#aggregate(lambda tdf: tdf.unique().tolist()).nunique\n","df2.to_csv(\"rapid_test_BB_match_%s_trimmed_bbcount.csv\"% barcode)#\"decode_\"+barcode+\"_2MM_bbcounts.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H6S5OATgFozg"},"outputs":[],"source":["# Group by Encoding regions (ER)\n","df = pd.read_csv('rapid_%s_BB_match_trimmed.csv' % barcode)#(\"decode_\"+barcode+\"_2MM_trimmed.csv\")\n","df2 = df.groupby('sequence').agg({\"BB code\": 'unique',\n","                                 \"UMIcount\": 'sum',\n","                                 'Line#':'nunique',\n","                                 'ER1':'unique',\n","                                 'ER2':'unique',\n","                                 'ER3':'unique',\n","                                 'ER4':'unique',\n","                                 'ER5':'unique',\n","                                 'ER6':'unique'})\n","df2.to_csv(\"rapid_%s_BB_match_byBead.csv\"% barcode)#\"decode_\"+barcode+\"_2MM_byBead_n.csv\")\n","#df3.to_csv('CGATGT_decode_byBead_n2.csv')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tkZVqVi-8CPR"},"outputs":[],"source":["display(df2)"]},{"cell_type":"markdown","metadata":{"id":"GBo3rg2GidvB"},"source":["##Assess reads for recombination\n","  \n","  If sequences for beads appear with the same encoding regions despite a diversity of encoding regions used to prepare the library, throw out reads which are improbable due to sampling statistics and are more likely a product of recombination during sequencing."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sZaXn-fIdQ8I"},"outputs":[],"source":["import ast\n","from ast import literal_eval\n","from math import log\n","\n","from scipy.stats import poisson\n","# barcode = 'CGATGT'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fEEs5tbAPgsE"},"outputs":[],"source":["def hamming(s1, s2):\n","    if len(s1) != len(s2):\n","        raise ValueError(\"Undefined for sequences of unequal length\")\n","    return sum(ch1 != ch2 for ch1, ch2 in zip(s1, s2))\n","\n","\n","\n","\n","df=pd.read_csv(\"rapid_%s_BB_match_byBead.csv\"% barcode)\n","print(df)\n","# df['AggER']=df['ER3']+ \"-\"+ df['ER4'] + \"-\"+ df['ER5'] + \"-\"+ df['ER6']\n","df2 = df.groupby('BB code').agg({\"sequence\":', '.join,\n","                                 \"UMIcount\": 'sum',\n","                                 'Line#':'sum',\n","                                 'ER1':'nunique',\n","                                 'ER2':'nunique',\n","                                 'ER3':'nunique',\n","                                 'ER4':'nunique',\n","                                 'ER5':'nunique',\n","                                 'ER6':'nunique'})\n","# display(df2)\n","for i, row in df2.iterrows():\n","  # print(\"original\")\n","  code = row['sequence']\n","  # print(code)\n","  if len(code) > 36:\n","    code = ast.literal_eval(code)\n","    uniquechunk=[]\n","    uniquepiece=[]\n","    samechunk = []\n","    samepiece = []\n","    for piece in code:\n","      chunk = piece[2:]\n","      # for base in uniquechunk:\n","      #   print(hamming(piece, base))\n","      if all(hamming(chunk, base) >= 1 for base in uniquechunk):\n","        uniquechunk.append(chunk)\n","        uniquepiece.append(piece) #if there is a diff in codes, pass\n","      else:\n","        count +=1\n","        samechunk.append(chunk)\n","        samepiece.append(piece)\n","      # poisson\n","    lam = len(code)/4\n","    k = len(samechunk) #reps\n","    p = poisson.pmf(k=k, mu=lam)\n","    count = 0\n","    if p < 0.05:\n","      print(samepiece)\n","      for line in samepiece:\n","\n","        values = ', '.join(str(v) for v in line)\n","        values = \"[\" + values+ \"]\"\n","        # print(values)\n","        # print(values)\n","        df.drop(df.loc[df['sequence']==values].index, inplace=True)\n","        # df = df[df.sequence != values]\n","\n","\n","\n","print(df)\n","df.to_csv('rapid_%s_BB_match_byBead_recomb.csv'% barcode)"]},{"cell_type":"markdown","metadata":{"id":"suxeykC_Y3bo"},"source":["## Sort hits by k-class or replicate class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AhrgZ_3REoVk"},"outputs":[],"source":["df = pd.read_csv('rapid_%s_BB_match_byBead_recomb.csv' % barcode)\n","df2 = df.sort_values(['UMIcount'], ascending=False)\n","\n","df2 = df2.groupby(['BB code']).agg(Reps = ('sequence', 'nunique'),\n","                                   sumUMI = ('UMIcount', 'sum'),\n","                                   ER1   = ('ER1','nunique'),\n","                                   ER2   = ('ER2','nunique'),\n","                                   ER3   = ('ER3','nunique'),\n","                                   ER4   = ('ER4','nunique'),\n","                                   ER5   = ('ER5','nunique'),\n","                                   ER6   = ('ER6','nunique'))\n","# print(df2)\n","cols = ['ER1', 'ER2','ER3','ER4','ER5','ER6']\n","df2['nERs'] = df2[cols].apply(lambda row: '-'.join(row.values.astype(str)), axis=1)\n","df2 = df2.sort_values(['Reps'], ascending=False)\n","# print(df2)\n","df2.to_csv(\"rapid_%s_BB_match_codesByBeadsorted_recomb.csv\"% barcode)\n","display(df2)"]},{"cell_type":"markdown","metadata":{"id":"BGGCKOYsmm6G"},"source":["#Aggregate data from negative (ortogonal control)screens"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VC4odC1NZlDw"},"outputs":[],"source":["# obtain all relevant files\n","\n","dir = \"/content/drive/MyDrive/Colab Notebooks\"\n","files = glob.glob(dir+ \"/*_codesByBeadsorted_recomb_trimmed.csv\")\n","print(files)\n","# print(files[4])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2nzGm2PuexHc"},"outputs":[],"source":["posneg = 4 #index of negative file\n","df1 = pd.read_csv(files[posneg])\n","print(df1)\n","df = pd.DataFrame(df1)\n","# df=df.drop(['ER1', 'ER2', 'ER3','ER4','ER5','ER6'], axis=1)\n","df = df.set_index(['BB code'])\n","# print(files)\n","columns_to_join = ['Reps', 'sumUMI', 'nERs']\n","\n","for i, file in enumerate(files):\n","  if i ==posneg:\n","    continue\n","  base=os.path.basename(file)\n","  file_name = os.path.splitext(base)[0]\n","  # file_name = file_name.split(\" \")[0]\n","  print(file_name)\n","  df_add = pd.read_csv(file)\n","  df_add = df_add.set_index(['BB code'])\n","\n","  # print(df_add)\n","  df = df.join(df_add[columns_to_join], how='outer', rsuffix = str(\" \"+ file_name))\n","  # print(df)\n","df['in negative?'] = df['Reps']>=2 # replicate class cutoff for negative screen\n","\n","#find all columns with replicates from the different barcodes and sum\n","\n","reps_columns = df.filter(like='Reps ') ## will only take reps from positive screens\n","df['total_reps'] = reps_columns.sum(axis=1)\n","df['appearance, nscreens'] = reps_columns.count(axis=1)\n","\n","#find all columns with sumUMIs from the different barcodes and sum\n","\n","UMI_columns = df.filter(like='sumUMI ')\n","df['totalUMIs'] = UMI_columns.sum(axis=1)\n","\n","#find all columns with unnamed indexes from the different barcodes and drop from df\n","\n","drop_cols = df.filter(like='Unnamed')\n","df = df.drop(drop_cols,axis=1)\n"," #sort the DF to put all relevant info together\n","df = df.reindex(sorted(df.columns), axis=1)\n","df.to_csv('agg_data.csv')\n","display(df)"]},{"cell_type":"markdown","metadata":{"id":"3Dh1pm3jh16H"},"source":["# Model full products\n","\n","##Code has been omitted from this portion to retain confidentiality of hit data##\n","\n","Code will utilize aggregated data and based on SMILES, generate full products and truncates based upon the library build, retaining information regarding Unique UMI's per hit, appearance in *n* screens, and encoding region health."]},{"cell_type":"markdown","source":["#False discovery rate calculations\n","Calculate FDR for k class cutoff determination based on poisson sampling statistical probability"],"metadata":{"id":"OI7wiiIfbhaV"}},{"cell_type":"code","source":["import ast\n","from ast import literal_eval\n","from math import log\n","\n","from scipy.stats import poisson\n","# barcode = 'CGATGT'"],"metadata":{"id":"zFtD4qHG-iIS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# for n in range(5348):\n","\n","lam = 479/3456\n","for k in range(8):\n","  # k = 0 #reps\n","  p = poisson.pmf(k=k, mu=lam)\n","  if k == 0:\n","    print(k,\" prob:\" , p)#)\n","  if k > 0:\n","    print(k,\" prob:\" , p)\n"],"metadata":{"id":"gD4z2wRMbfus"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":["NACGovWFu-UM","pAN8IV3HvB41","BAh-1Ki21C-J","ajiWTCgRQ6pb","K9vClu1PyGSX","Nq5-Zeh-mkNL","mX3hH4T-tteD","cf2ZEk8s7GVC"],"provenance":[{"file_id":"1IVbcQQ4BH3dcHwBnuhavwbbb6hm3fRch","timestamp":1719439385234},{"file_id":"1y6eaBffaWqMdvxO0yci6TKao1nygeb8G","timestamp":1708891316169},{"file_id":"1JJzH1iDg93OO9A8-f-bbkVV5rbdNLWAS","timestamp":1708127882456}],"authorship_tag":"ABX9TyMzPTJYH4ELJ4lpDuT8yhTZ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}